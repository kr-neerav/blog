+++
title = 'Ch01'
date = 2024-10-11T05:30:36-07:00
draft = true
+++
## Data lifecycle
### Collect
* first step in the design process is ingestion. ingestion refrers to the process of transferring data from source to a target system where it can be stored for further analysis.
* The questions to understand for this stage are volumne, velocity, variety of data and how to get data from source (push/pull based model), connectors required.
* variety of data is structured, semi structured or unstructured
* frequency of data is continuously or at specific intervals. based on the velocity choose between batch ingestion or streaming ingestion or a hybrid of the two.
* this stage should be flexible to expand to different data sources e.g. tools to transfer flies from FTP to collect data from IoT.
### Store
* just store raw data as you might want to process the data in a different way later and you need to hvae the original data to do that.
* data comes in many different forms and sizes. The way you store it depends on your technical and commercial needs. Common options like object store, RDBMS, DWH, data lake.
* scalability:underlying system should be able to cope up with the volumne and velocity required. data is exploding and its nature is transforming from batch to real time. people are generating more data in real time and requiring access to it in real time.
* performance vs cost: identify different types of data you need to manage i.e. a hierarchy based on business important of data, how often it will be accessed and what king of latency the users of the data will expect. Important and frequently accessed data(hot data) is stored in a high performance storage system like a DWH, less important data (cold data) in a less expensive object store. If you need even higher performance for interactive use cases use caching techniques to load meaning portion of your hot data into a cache.
* high availability: store data in multiple availability zones to cope up with physical failures or outages.
* durability: ability to store data long termwithout suffering data degradation, corruption or loss. achieved by usually storing multiple copies of data in separate regions. important in the case of data restore operations in the face of natural disasters.
* openness: use formats that are not proprietary and do no generate a lock in. it should be possible to query data with a choice of processing engine without generating copies of data or having to move it from 1 system to another. e.g. delta lake stores data in proprietary format (delta lake) to enable ACID transactions on data lake. This is costly to do on top of parquet using EMR. Hence that lock in might have enough ROI to still consider. An important factor is that the solution should support exporting to open format like parquet if a decision is make to migrate from it in future or move data to a different system for different processing.
### Process/Tranform
* raw data is transformed into useful information for further processing.
* data integration involves combining data from multiple sources into a single view.
* data cleansing may need to remove duplicates and errors from data.
* data transformations are carried out to organize data into a standard format.
* engines that allow you to query and transform data using SQL are efficient and cost effective. efficient as its declarative and optimizer identifies the best way to process the data. cost effective as SQL is a common language which is widely used and requires little rampup. examples include AWS Athena
* however SQL based offer limited capabilities compared to programming based engines like Scala, Python (Spark, Flink, Beam). they allow you to implement more complex transformation or ML and  support features like unit and integration tests.
### Analyze/Visualize
* At this stage the data starts to have value in itself and is considered information. users leverage different tools to dive into data to extract useful insights, identify current trends and predict new outcomes.
* visualization tools play an important role as they provide an easy way to discover trends, patterns, behavior.
* different type of users e.g business users looking to access data via dashboards, power users who leverage SQL to do tailored analysis and data scientists who can leverage ML techniques.
### Activate
* end users make decisions based on data analysis and ML predictions, thus enabling data driven decision making. There are 3 type of actions.
* automated actions: recommendation system provide customized recommendations
* business decisions: using historical/current data to analyze trends to take up business decisions.
## Limitations of traditional approaches
* organizations consistent of independent data solutions that are used to provide different data services. however such task specific data stores can lead to creation of silos.
* if majority of the solutions are custom built it becomes difficult to handle scalability, business continuity, governance. one solution is to build a unified data platform. unified does not necessarily imply centralized.
* the purpose of a data platform is to allow analytics and ML to be carried out across org's data in a consistent, scalable, and reliable way. To do this we should leverage managed services to the extent possible, so the org can focus on business needs instead of operating infrastructure.
### Antipattern: Breaking down silos using ETL
* data quality: ETL pipelines are written by consumers of the data who tend not to understand it as well as the owners of the data. hence data extract might not be right or transformed correctly.
* latency: ETL tools introduce latency as they run at scheduled intervals.
* bottleneck: ETL tools involve programming skills, hence orgs need data engineering teams
* maintenance: maintaining infrastrucutre and pipeilnes
* change management: upstream changes require etl pipelines to be updated
* governance: ETL processes proliferate and likely that same processing is being carried by different sources, leading to multiple source of same information. over time they diverge and lead to inconsistencies.
### Antipattern: Centralization of Control
* to address data silos and distributed data mangement org try to centralize everything in a single monolithic platform under control of it. The underlying technology doesn't changes instead problems are more tractable by assigning them to a single org.
* IT: face challenge with the diverse set of tech involved, costly to manage different infrastructure like DWH, data lake, data mart. Also not clear how to define effective data governance across data silos.
* Analytics: one of the main problems hindering effective analytics is not having access to right data. either data does not exist in the golden sources or is not in the right format. it is impossible to give your analytics team access to all data due to governance challenges. organization often end up limiting data access at the expense of analytic agility.
* Business: often data provided to business needs to be of high quality. this results in less data being exposed to them to ensure high quality. organizations work around this by creating their own teams/org to manage data thus resulting in data silos.
### Antipattern: Data Marts and Hadoop
* DWH were created but they acted more like data marts as they could not scale to enterprise data. Open source technologies like Hadoop helped created scalable storage and compute solution which resulted in creating data lake that could scale to enterprise data. however these data lake became ungoverned mess of data that a few potential users of data could understand.
## Creating a unified analytics platform
### Drawbacks of Data Marts and Data Lakes 
* DWHs are hard to scale, not easily accessiable for AI/ML or real time features.
* DWH users are frequently analysts that only have read only access thus limiting their ability to innovate by creating their own versions of datasets or limiting their ability to test new hypothesis.
* On premises Data lakes scale easily but orgs face challenges in planning and provisioning sufficient storage. Same is the challenges with provisioning compute compacity for peak periods.
* On premises data lake require maintenance from engineers which is increases total cost of ownership thus reducing ROI.
* Governance is not easily solved in data lake with different parts of the organizations use difference security models. data lake becomes siloed and segmented making it difficult to share data.
* Data lake require programming skills to access, thus making it difficult for business users to access this data and requiring a set of developers to derive insights from it.
* building dashboarding solutions like Clarity helps business users get access to a lot of insights without the need to work with engineers. For a few scenarios that they don't get insights they need to work with developers. This scenarios is a significant improvement over business users having to depend on developers for any/all insights.
### Convergence of DWH and Data Lakes
* companies end up taking a mixed approach of having both Data Lake and DWH, where some data is grduated to DWH and all data is kept in Data lake.
* to drive greater use of data orgs are moving towards lakehouse and data mest.
* lakehouse allows users with different skillsets (data analysts and engineers) to access the data using different technologies.
* data mesh allows an enterprise to creat a unified data platform without centralizing all data to one team (like BDT), thus different business units can own their data but allow  other units to access it in an efficient, scalable way.
#### Lakehouse
* it combines key benefit for DWH and data lake like cost cost storage, access by various processing enginers such as SQL engine for DWH, providing data management and optimization features.
* it decouples storage and compute for scalability
* provides ACID compliant storage opertaions
* data governance like access restriction and schema evolution
* integration with BI tools and ML applications
* support for open formats like parquet and iceberg
* streaming capabilities with the ability to handle real time data
* SQL supported by DWH is more efficient than SQL supported by lakehouse as DWH uses properiety storage optimized for its operation where as lakehouse uses open table formats.
#### Data Mesh
* decentralized operating model to solve common challenges in analytics
* it decentralizes data ownership among domain data owners each of whom are held accountable for providing their data as a product in a standard way.
## Applying AI
* when designing a data platform, it is important to ensure that it will be future proof in being capable to support AI use cases.
* real time personalized ML is there the value is. to take advantage of these opportunities data platform must be able to ingest, process and serve data in real time for opportunities are lost.
* operationalizing ML is hard. success ML projects require operationalizing both data and code.
## Core Principles
* defining key principles and assigning weights to each helps make trade off decisions between those principles.
* Deliver serverless analytics, not infrastructure: minimize the need for data consumers (data engineers, analyts, scientists) to have custom infrastructure to process data and provide managed services like AI workbench. This will enable them to focus on solving their business problem instead of managing infrastructure.
* Embed end to end ML: allow orgs to operationalize ML end to end. It is impossible to build every ML model that org needs, so building a platform within which it is possible to embed ML options such as pre built models, ML building blocks, easier to use framework. Ensure ML ops is supported.
* Empower analytics across the entire data lifecycle: data platforms offer core data analytics workflows. it offers data storage, data warehousing, streaming data anlytics, data preparation, big data processing, data sharing and monetization, BI and ML. Avoid buying one off solutins that you will have to integrate and manage.
* Enable Open Source Software (OSS) tech: whenever possible ensure open source is the core the data platform.This will help provide flexibility and choice in data nalytics projects.
* Build for growth: data platform should be able to scale to the data size, throughput, concurrent users. 
* The principles are listed in order of priority.
* 
