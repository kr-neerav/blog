+++
title = 'Ch05'
date = 2024-12-30T10:45:28-08:00
draft = true
+++
* Data lake captures raw, ungoverned data from across the org and supports multiple compute tools.
* One of the key benefit of data lake is its ability to store all forms of data i.e. structured and unstructured at low cost.
* benefit of using a data lake in cloud is 1) low cost storage 2) decoupled compute and storage 3) integration with multiple compute options
## Design and Implementation
* depends on whether you need batch or streaming, how you will manage data governance and what compute options you will use.
* whether batch or streaming there are 4 storage areas in a data lake
* raw/landing zone: raw data is collected and ingested from source
* staging/dev zone: for advance users like DE or scientists to process data to prepare for final users.
* production: final data to be used by users
* sensitive (optional): sensitive data resides  to facilitate governance and compliance
* in kappa architecture there are separate layers for batch and real time which are unioned during consumption or queried independently. This is because the underlying storage layer is unable to support both batch and streaming ingestion.
* Data is stored across multiple systems like data lake, dwh and hence need a catalog for discoverability. in addition it is also stored across different areas of data lake.
* making data discoverable is the first step to breaking down silos and deduplication
* whenever data is shared across an organization there should be data contracts between data producers and consumers. These should be in JSON/YAML for programmatic audit.
## Integrating the data lake
* data lake interaction takes place through API. this allows different systems to connect with data lake either for ingestion or for reading data and publishing.
### Evolution of data lake through Apache Iceberg, Hudi and Delta lake
* Apache Iceberg supports more format than Hudi or Delta Lake
* ACID compliance giving users certainity that information they are querying is consistent.
* overcoming inherent limitations of HDFS interms of file size, even small files can work well.
* logging every change on data guaranteeing auditability
* No difference in handling batch and streaming ingestions
* storage based on parquet format that enables high compression
* applying governance at row/column level
* These capabilities can enable DWH use cases on data lake mainly because of the ability to prevent data corruption, execute queries in fast mode and frequently update data.
* the transactional feature is helpful for compliance with GDPR and other regulations i.e that required OD3. Earlier it requires all files with personal data to be rewritten as new files and the original ones deleted. this format simplifies this operation and makes it reliable.
* partition evolution in Apache Iceberg: changing the partition scheme of the table over time. in Hive environment this requires rewriting the files again. How does it happen with Apache Iceberg?
* schema evoluation in Apache Iceberg: ability to update schema without retransforming the data.
### Interactive analytics with notebooks
* one of the important aspect of dealing with data is to interactively get access to it and perform analysis in an easy and fast manner. one of a famous solution for this is through notebooks that act as live documentation containing text, code, charts, output etc.
* Jupyter notebook architecture is notebook -> Kernel -> Data Lake/HDFS. The kernel can be Pyspark/Spark for spark based notebook.
* notebook is the first step in the journey to democratize data.
### Democratizing Data Processing and Reporting
* the best value data provides to organization is that it enables decision makes to make informed decisions. data should be accessible and usable to any authorized individual without need for adhoc expertise.
* good metadata will enable users to find the riught information they need
* good data governance will ensure only authorized users and use cases get access to data.
#### Building trust in data
* end users need to undertsand the source of data, transformations it has undergone. This trust is essential for customers to use the data.
* initially all these questions were answered by IT dept. However nowadays final users have tools that enable them to answer majority of questions in a self service manner
* tools like AWS Glue are making the process of metadata collection easier e.g. through crawlers the metadata about files in data lake are generated and made available to users.
* data processing, preparation, cleaning and wrangling steps can be handled by end users through tools like AWS Sagemaker Studio. Some complex data processing might still require DE expertise but common tasks can be handled and metadata for these are captured and made available for users.
* reporting/visualization can be carried out using BI tools that end users are already familiar with.
### Data Ingestion is still a matter of DE
* organizations tend to load every kind of data in data lake and it can easily become a data swamp with unused, stale and incomplete data that costs the organization.
* best practices during ingestion process
    * file format: for readability use CSV, JSON, XML. For performance using ORC, Parquet, Avro. Avro works better for intense I/O and low latency operations like streaming dashboards. parquet and ORC are suitable for query use cases.
    * file size: in hadoop larger file size is better. when working with streaming data use a streaming engine to consolidate data into few large files.
    * data compression: very important to save space. for performance the compression algorithm should be fast enough to compress/decompress data on the fly and be splittable. Snappy is an example.
    * directory structure: to enable partition prunning
* ingestion involves a lot of automation and at the same time lot of integration/data processing which require DE expertise.

