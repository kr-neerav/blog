+++
title = 'Ch02'
date = 2024-10-15T21:05:40-07:00
draft = true
+++
* the reason for building a data platform is because organization wants to innovate. Innovation typically happens to through better understanding of customers, products or the market.
* Data is needed to analyze current state of the business, identify shortcomings or opportunities, implement ways to improve upon status quo and measure the impact of those changes.
* 7 steps to take to build a data platform
## Strategy and planning
### Strategic Goals
* stakeholders will frame goals in terms of the limitations of the current platform. However this will build your platform on narrow use cases. Instead design your system based on the strategic direction of the business e.g. growth of customers, new use cases, personalization. The design should also address the current limitations while unblocking new business.
* This unblock you from being tied down to a limited number of use cases and old ways of thinking It also helps you communicate the need for the system to non technical executives.
* always consider growth of data over next few years to not make short sighted decisions.
* Some common strategic goals
    * Reduce cost of operating data and AI platforms: linearly growing IT costs with growth in dataset size can be unsustainable
    * Increase speed of innovation by accelerating the time to get value from new projects: Experimenting on new ideas should not face delays on procuring data, infrastructure
    * Democratize insights from data: Enable domain experts and people in the field to interact with the data directly and gain insights
    * Incorporate predictive analytics, not just descriptive analytics into decision making
### Identify Stakeholders
* A solid definition of the strategy strats with the correct requirements gathering. To do this it is important to identify the right people within the org.
* data transformation journey has the highest rate of success when supported directly by the business.
* it is not just a matter of technical knowledge but also a matter of leadership and management to ensure that the design and implementation of the data platform are successful and that the business outcomes are positive.
### Change management/Execution
* Need to consider the cultural aspect of the transformation as well and not just treat it as a technological project.
* upskilling of people is required to the new capabilities
* leadership support is important for adoption
* automation is critical for success as it reduces human effort and makes low-risk and frequent changes possible that are key ingredients for innovation.
### Reduce total cost of ownership
* This is a good value proposition in terms of IMR cost, maintenance cost.
* Workloads that benefit from moving to cloud
    * spiky and ephemeral workloads as it reduces the need to spend time doing resource provisioning, leverage autoscaling, pay for what you use.
    * workload that can benefit from scaling out i.e. a job running for 10 hours can be scaled to run in 30 minutes by scaling to larger infrastructure. This results in monthly jobs being run daily, thus increasing speed of innovation.
### Break Down Data Silos
* data silos is having multiple, disjointed and sometimes invisible datasets.
* breaking down data silos is to decentralize ownership of data to domain owners as they can ensure quality is high.
#### Unifying data access
* What does not work is to have each team put its data on a cluster and then manage acess to that cluster. data quality reduces further away from the domain owner you get. Centralize data vending to a single platform but ownership of data is decentralized.
* There should be a single auth mechanism instead of multiple. So database specific credentials should be replaced with a proxy.
* ensure access to data is auditable i.e. its based on the actual user making the request and not the srevice accounts that break the link back to individual user.
* store data in open table format such as parquet using distributed table format such use Apache Iceberg or Delta Lake, on top of an object store. This can hit the SQL performance a little when compared to proprietary storage of DWH, but the lower cost of storage and flexiblity to support non SQL analytics like ML make this a worthwhile tradeoff.
* unstructured data should be stored in a format and location where it can be processed by multiple compute engines. use standard file formats like parquet.
* semantic layer is used if analytic teams create similar definitions of metrics and we need a unified layer for customer definitions.
### Make decisions in context faster
* the value of a business decision decreases with latency and distance.
* if a decision is make in 1 minute vs 1 day, the 1 minute scenario is more valuable.
* similarly if you are able to make a decision based on the user context i.e. where they are, what they are doing then it is much more valuable that generic decisions. An example here is personalization.
* as size of data increases it starts to make sense to process data more frequently. This helps reduce blast radius of failures, get data to business faster. 
* automation is another way of speeding up decision making. as business focus on the long tail of customers there is an increasing need to cut down on friction in user experience. A frequent, expert user of your product will put up with a lot more than an accasional less sophisticated user. Being able to catch frustrated users and provide them contextual help becomes important.
* real time, location based visualizations are increasingly how decisions are made. Ensuring your schema has location details upfront is important as its very difficult to add retroactively add clean geographic locaiton.
* streaming has a reputation of being expensive to implement, monitor and maintain. To address this avoid building 2 systems one for batch and one for streaming to reduce the infrastructure cost. Also do not bulid custom monitoring, observability, scaling and so on.Using existing capabilities. Skills to manage aand troubleshoot streaming infrastructure are rare.
* an alternate to streaming is micro batching. This works when users don't require data in real time but fresh data is enough.
### Leapfrog with packaged AI solutions
#### Predictive Analytics
* ML models are trained on correct decisions and DWH is often a great source for that.
* Use integrations with DWH to use these models e.g. AWS Sagemaker
#### Understanding and Generating Unstructured Data
* unlike predictive analytics it is rarely necessary to create and train models to understand unstructured data. Instead pre built models like be used as it is e.g. AWS Comprehend
### Operationalize AI Driven Workflows
* automating end to end workflows to solve business problems in a fully automated and autonomous way.
* example you don't want to predict when a machine will fail but want to schedule a maintenance for the machine before it fails.
* Predictive Analytics is assistance, taking action on the insight is automation
* the right balance of this is decided by each organization and varies. There is no right answer to this. However this balance is used as a competitive advantage by business.
* it is not enough to simply put in place an analytics data platform. it is also necessary to change the culture of the org to one where data driven experiments are the norm.
* just because you build a platform that enables data silos to be broken doesn't mean that they will be. Just because data driven decisions can be made does not mean old heuristics will be easily discraded.
* successful orgs adopt a transformation program to provide training on data tools, align mechanisms like measuring adoption.
* 
