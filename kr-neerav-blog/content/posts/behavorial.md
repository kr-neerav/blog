+++
title = 'Behavorial'
date = 2025-02-02T17:42:35-08:00
draft = true
+++

## Introduction
Thank you for the opportunity to interview at XX. I have been working as a Senior Data Engineer in Amazon for the last 6 years and been with Amazon for last 10 years. I work in Talent Management Domain in HR space where I support analytsts and scienctists to help our leaders make data driven decisions for Amazon Employees. Some of my key achievements are creating a data vision for the org to improve data governance and speed of analytics for the org that impacted more than 8 teams. Led the development of the data governance strategy for the org and currently leading its implementation. Our team launched the first Retreival Augment Generation data service for an LLM that was used as a chatbot exposed to all Amazon employees. I am trusted by my managed to handle ambiguous or complex situations, ensure we solve the right customer problem and collaborate with other teams. I am interested in this role because XX. I believe my skills and experience make it a strong fit for this position.

## Why looking for a change?
I have learnt a lot at Amazon from data engineering, to data management at scale, to collaborating effectively across teams to solving the right problems. There are 2 reasons why i am looking for a chage. First my whole team is in Seattle and my management has shared that there is a good chance that I need to move to Seattle by next year. Since I want to stay in Bay Area i have a motivation to look for a new opportunity. Secondly I am seeking a role with bigger challenges and more responsibility. Reading the job description I felt this role provides that.

## Why are you interested in this role/company?
### Reddit
As a user of Reddit I have found it very helpful to deep dive on topics and get inputs from a community specific to my needs. I see it a platform build for authentic user communications. I will like to contribute to its mission to maintain the thriving community and be the source of valuable information for users. Also my past experience of supporting data driven decision making, working with product managers, scientists and analytics, build the data architecture for the org to improve the effectiveness of its decisions seems to be relevant to this role and I believe I can have good impact in this role. 




## Disagree and Commit
I was part of a working group tasked with defining a data strategy to reduce exposure to sensitive employee data and improve the data consumer experience.  The principal leading the group proposed a secure data vending strategy. While I understood the need for security, I had concerns about the potential impact on customer experience.  My analysis suggested this approach changes they way customer perform analytics in the form of increased context and reduced knowledge sharing, potentially impacting customer engagement. I also felt the proposed approach created significant overhead for our team to maintain. 

My role was to represent the data engineering and analytics community.  I believed data governance is a spectrum and there are other points in that spectrum that we can be at that can better serve our needs.  To articulate my concerns, I created a one-page document comparing the secure data vending strategy with an alternative approach that prioritized both security and customer experience.  I presented this document to the principal, highlighting the potential customer impact and cost implications of their proposal. I also explained how my proposed approach could achieve a similar level of security with less impact on usability.

While the principal initially disagreed, I didn't stop there. I actively listened to their concerns, which primarily revolved around aligning with privacy guidelines on handling employee data.  I then suggested we engage our privacy specialist to assess both approaches from a data governance perspective.  I also reached out to several data engineers across the organization to gather their feedback on the proposed strategy.  This feedback confirmed my concerns about the customer experience impact.  I then shared this consolidated feedback with the principal, demonstrating broader support for a more balanced approach.

As a result, the principal agreed to revise the strategy.  For analytics use cases, we adopted the options I had proposed, which balanced security with the need for agile data access.  For application data exchange, we proceeded with the secure data vending approach, as it was deemed necessary for those specific use cases.

Looking back, I learned the importance of proactively engaging key stakeholders early in the process.  If I were to do it differently, I would have involved the privacy specialist and other data engineers earlier. This would have allowed us to address the principal's concerns about regulatory risk more directly and incorporate broader feedback into the strategy from the outset.  It would have also allowed me to build consensus earlier, potentially leading to a smoother decision-making process.

## Earn Trust
### Tough or critical feedback you received
We were experiencing recurring data quality issues that were impacting our customers.  To address this, my team had developed a basic data quality monitoring solution using AWS Deequ.  A sister team was facing a similar challenge, and my manager asked me to explore if they could leverage our solution.  Initially, I approached this as an opportunity to 'sell' our solution.  I set up a meeting with them and focused on highlighting its features, often countering their own approaches with how our solution could achieve the same results.  I realized later that this approach was counterproductive.

My manager provided valuable feedback, pointing out that I hadn't adequately considered the other team's perspective and needs.  This feedback was crucial. It made me realize that I had prioritized promoting my team's work over understanding the other team's context. I learned that effective collaboration requires empathy and a genuine desire to understand the other party's challenges.

I took the feedback to heart and scheduled another meeting with the sister team.  I started by openly acknowledging that my previous approach had been misguided and that I was there to understand their situation and explore how we could work together.  I listened carefully as they explained their current data quality monitoring setup, the challenges they faced, and the specific requirements they had.  I learned that they had already invested time and effort in exploring different solutions and were hesitant to adopt something new without a clear understanding of its benefits.

Instead of 'selling,' I focused on finding common ground. I acknowledged the work they had already done and expressed genuine interest in their approach.  I then explored how our existing solution could complement their efforts and potentially save them time and resources.  I proposed a collaborative approach where we could jointly enhance our solution to address both our needs.  I emphasized that their contributions would be invaluable in making the solution more robust and adaptable to different use cases.

This collaborative approach was much more successful.  We jointly enhanced the data quality monitoring solution, incorporating features that were important to both teams.  As a result, the solution was used for over 50 datasets (30+ from our team and 20+ from the sister team), significantly improving our overall data quality.  This collaboration also fostered a stronger working relationship between the two teams.

Looking back, this experience taught me a valuable lesson about the importance of empathy, active listening, and genuine collaboration when working with other teams.  If I were to do it differently, I would have started by focusing on understanding their needs and building rapport before even discussing our existing solution. This would have created a more collaborative environment from the outset and likely led to a more positive outcome.

### Not able to meet a commitment
We had a serious security incident where a customer accessed sensitive employee data for an unapproved use case.  This was escalated to the VP level and revealed a systemic issue: we lacked a process for baselining data access across the organization.  Leadership tasked us with driving an initiative to baseline access for all 340 datasets and 160 dashboards.

We initially estimated the project would take three weeks – one week for preparation and two weeks for execution.  However, this estimate proved to be significantly inaccurate.  In retrospect, I realize we made several critical planning errors.  We hadn't fully accounted for the complexity of coordinating with multiple teams, the variability in customer responsiveness, and the potential for conflicting priorities.  We also underestimated the effort required to create a comprehensive access baseline framework.

When we developed a more detailed project plan, we quickly realized our initial estimate was unrealistic.  We proactively communicated this to leadership, explaining the reasons for the revised 5-week timeline.  While they were understandably concerned about the delay, they appreciated our transparency and the thoroughness of the revised plan.

However, even the 5-week estimate proved optimistic.  The actual execution took 12 weeks.  We encountered several challenges:  delays from customers in recertifying access, conflicting priorities within other teams, and a general lack of preparedness for this type of organization-wide initiative.

To address these challenges, we took several steps.  We established a clear communication channel with each team, assigning a point of contact to facilitate the process.  We worked with team leads to understand their competing priorities and negotiate timelines that were feasible for everyone.  For teams that lacked preparedness, we provided training and documentation on the access baselining process.  We also escalated critical roadblocks to management when necessary, seeking their support in resolving conflicts.

Throughout the process, we maintained weekly updates to leadership, providing not just status reports but also highlighting emerging risks and challenges and the mitigation strategies we were implementing.  This transparency helped us maintain their confidence, even when the project timeline slipped.

Despite the challenges, we successfully completed the access baseline.  More importantly, we learned valuable lessons about project planning, risk management, and cross-functional collaboration.  We documented our best practices and lessons learned, which were incorporated into the organization-wide data strategy.  Specifically, this experience highlighted the need for a centralized access management platform to automate the baseline process, which became a key recommendation in the data strategy.

Looking back, if I were to do it differently, I would have conducted a more thorough initial assessment, including detailed discussions with representatives from each team, to better understand the potential challenges and dependencies.  I would have also built in buffer time for unforeseen issues and established clearer communication protocols from the outset.  This experience reinforced the importance of proactive risk management and the need to adapt plans as new information becomes available.
### Uncovered a significant problem in the team/ improved team's productivity
Our team was consistently struggling to complete tasks within our sprints, resulting in a punt rate between 20% and 45%. This directly impacted our ability to meet committed customer timelines, leading to several instances of missed deadlines.  I was concerned about the impact on customer trust and decided to investigate the root causes.

I analyzed data from the past eight sprints, including task completion rates, reasons for punting, and feedback from sprint retrospectives.  I discovered two key issues:  First, team members were frequently starting work on stories before they were fully groomed, leading to confusion about requirements and scope creep. Second, our sprint retrospectives were ineffective, focusing on generic challenges rather than specific actions to prevent future issues.

To address these issues, I created a two-page proposal outlining my findings and proposing concrete changes.  I presented this proposal to the team, explaining how these changes would directly impact our ability to deliver on time for our customers.  I emphasized that by improving our planning and execution, we could significantly increase customer satisfaction.

My proposal included two key changes:  First, I introduced a new agenda for sprint retrospectives, including a dedicated section for each team member to analyze their punted tasks, identify the root causes, and propose specific actions to prevent similar issues in the future. Second, I worked with the team to define a clear "definition of done" for story grooming. This included specific criteria that had to be met before a story could be considered ready for sprint planning, such as acceptance criteria, detailed requirements, and dependencies identified.

After implementing these changes, we saw a dramatic improvement in our sprint performance.  Our punt rate fell to consistently below 10%. More importantly, this directly translated into improved on-time delivery for our customers.  We also received positive feedback from customers during our quarterly voice of customer meeting, with several customers specifically mentioning the improved timeliness of our deliveries.

This experience reinforced the importance of data-driven decision-making and the power of well-defined processes.  I continue to monitor our team's performance metrics and regularly solicit feedback from team members to identify areas for continuous improvement.
### Helped a team member who was struggling or impacted my work
Our team was tasked with developing a proposal to improve data quality, initially focusing on our own datasets with the intention of expanding the solution to other teams within the organization.  A junior engineer was assigned to lead this effort, working closely with their manager. However, their initial proposal overemphasized tooling solutions without adequately addressing the underlying root causes of our data quality challenges.  I recognized this as an opportunity to mentor the engineer and help them develop a more strategic approach.

Instead of directly providing a solution, I took the engineer under my wing and guided them through a structured problem-solving process.  I explained that before considering any tooling, we needed a clear understanding of the why behind our data quality issues.  I emphasized the importance of root cause analysis and how it would inform the selection of appropriate tools, if any.  I shared examples of how focusing on root causes could lead to more effective and sustainable solutions.

I then reviewed the notes from the initial interviews with other teams.  I pointed out that the discussions were primarily focused on desired capabilities for data quality monitoring, rather than diagnosing the current state.  I suggested that we needed to revisit those teams to gather more relevant information.

To help the engineer understand the type of information we needed, I facilitated follow-up meetings with two of the teams.  I structured the discussions around specific questions designed to uncover the root causes of their data quality challenges.  For example, I asked about the types of data quality issues they were experiencing, how they were currently detecting those issues (if at all), and what impact those issues were having on their work.  I showed the junior engineer how to actively listen, ask clarifying questions, and synthesize the information gathered.

As a result of these guided discussions, the engineer was able to collect significantly more meaningful insights.  We collaboratively analyzed this data and identified a key gap: a lack of clear understanding across the organization of what constitutes data quality and what metrics should be monitored.  This realization shifted our focus from simply implementing tools to defining a comprehensive data quality framework.

We revised the proposal to address this foundational issue.  The revised proposal defined different types of data quality issues, identified key data sources for understanding critical monitoring needs, and outlined various approaches for monitoring these issues.  Importantly, we realized that we could leverage existing tooling rather than building something new, saving significant time and resources.

We successfully implemented the data quality framework and monitoring on 35 of our own datasets and we block 10 key datasets from publishing if there are data quality issues encountered. We analyzed in 1 quarter we had 11 customer raised data quality issues. After this we had 4 customer raised data quality issues and a significant number of issues detected by our framework.  We then presented our approach and results to other teams in the organization, demonstrating the effectiveness of our strategy.  To date, we've helped one other team implement similar data quality monitoring programs. We are actively working with the remaining teams to implement this framework. This initiative has significantly improved the overall data quality posture of the organization, leading to more reliable data for decision-making and improved customer trust.

Looking back, if I were to do it differently, I would have been involved in the initial round of customer interviews.  This would have allowed us to identify the need for a root cause analysis earlier and potentially avoided the need for follow-up interviews.  I would have also set clearer expectations and provided a more specific template for the proposal from the outset, which would have streamlined the process.  However, the most valuable lesson learned was the importance of guiding and mentoring junior engineers, empowering them to develop the skills they need to succeed.
## Bias for Action
### Calculated risk where speed was critical
Our talent management analytics relied on a key dataset that hadn't been updated in four years.  When we finally introduced a change, a bug slipped through, causing a data quality issue that blocked our customers, some of whom relied on this data for fine-grained access control.  The situation worsened when we discovered that a recent production backfill had overwritten all backups, leaving us with no readily available data to restore.  We were facing a critical need to restore data quickly without a clear, documented process.

As a staff engineer, I took charge of the situation and immediately assembled a small working group to address the issue.  My first priority was to assess the risks. I considered several factors: the risk of further data corruption in downstream datasets due to an untested restoration process, the likelihood of introducing new bugs during the restoration, and the cost of delaying the restoration (in terms of business impact)

Given the urgency, I made the calculated decision to proceed with developing a simplified SOP for restoring the pipeline and data to its previous state. While this approach carried the risk of using an untested procedure, the potential cost of not restoring the data quickly was far greater.  To mitigate the risk of introducing new errors, I mandated a peer review of the SOP with two other engineers, including a senior engineer. We also developed small data testing scripts for each stage of the pipeline to catch any issues during the restore process.

We communicated our plan and the associated risks to stakeholders, including potentially affected customers, explaining the steps we were taking to minimize any further disruption.  The peer review and testing scripts proved invaluable.  We identified and corrected one data discrepancy during the restore process, preventing a potentially larger issue.  Ultimately, we were able to restore the data within two days, unblocking our customers.

Following the incident, I authored a Center of Excellence (CoE) document detailing the lessons learned and outlining improvements to our incident response process.  Specifically, we implemented a robust backup and restore strategy, including automated backups and regular testing of the restore process.  We also created a dedicated role within the team responsible for collaborating with the platform team on data restoration, as well as a more robust change management process.  This incident highlighted the critical need for better planning and risk assessment, which we addressed by implementing more rigorous project planning procedures.

Looking back, while the quick restoration was successful, I realized that we could have mitigated the customer impact even further.  If we had assigned an individual to work directly with the platform team to restore the last version of the final output, we could have potentially unblocked customers much sooner while we worked on fixing the root cause and preparing the pipeline for the next run.  This would have minimized the downtime and preserved customer trust.  This experience reinforced the importance of proactive planning, robust backup and restore procedures, and clear communication in critical situations.
### Decision without consulting your manager
We were working on defining child goals to support our science and analytics customers, particularly around experimentation.  While some goals, like streamlining experimentation analysis, were clear, others, such as creating North Star metrics for measurement, were ambiguous regarding ownership and scope.  I faced a dilemma: should we create a goal, even if it was poorly defined, or should we postpone it until we had more clarity?

I decided not to create the ambiguous goal at that time.  My reasoning was that setting a goal without a clear understanding of its impact, importance, and measurement criteria could lead to several negative consequences.  It could result in wasted effort, misaligned priorities, and ultimately, a failure to achieve the desired outcome.  I felt strongly that it was more important to invest time upfront to define the goal properly than to rush into setting a goal that was likely to be ineffective.  I considered the risk of delaying the goal setting, balanced against the risk of setting an ineffective goal.  I felt the greater risk lay in the latter.

Even though I made this decision independently, I informed my manager of my decision shortly afterward, explaining my rationale. They agreed with my assessment.

I then took the initiative to work with the science team to clarify the ambiguity surrounding the North Star metrics.  I facilitated a series of meetings with them, asking probing questions about their vision for these metrics, how they would be used, and who would be responsible for tracking them.  I also created a shared document where we could collaboratively define the metrics and their associated ownership.  Through these discussions, we were able to clearly define the scope and ownership of the North Star metrics, assigning responsibility for each metric to specific individuals within the data engineering team that supported that product.

This clarified ownership and definition led to a more focused and effective approach to measurement.  We were able to track the impact of our experimentation efforts more effectively, leading to a X% improvement in the success rate of our experiments.

This experience reinforced the importance of clear goal setting and the value of proactive communication.  If I were to do it differently, I would have perhaps engaged my manager earlier in the process, even if only to give them a heads-up about the potential ambiguity of the goal and my intention to address it.  However, I learned that sometimes it's necessary to make independent decisions based on sound judgment and then communicate those decisions transparently.  It also solidified my belief that investing time upfront to define goals clearly is crucial for long-term success.
### Tight deadline and didn't have time to consider all options
Customer problem: launching a chatbot for Amazon employees to ask compensation related questions. we owned creating the Retreival Augmented Generation Service to provide the LLM amazon specific compensation data.
Situation: We did a PoC on 3 Vector databases Opensearch serverless, Amazon Kendra and Amazon RDS with vector plugin. We finalized OSS due to its low cost of ownership  for our use case. During implementation Amazon Bedrock accounced an integrated RAG service Knowledge base. As a technical lead i was responsible for the data architecture. I know we don't have time to make a detailed assessment of Knowledge base before launching. I had to make a decision on continuing our path or moving to knowledge base.
Action: did a quick 1 day document read of Knowledge base. Realized there were still open questions related to support for text + vector searches, latency and TPS limits. it was hard to finalize this without doing an additional PoC. I recommended to use what we were already familiar with and which was well suited for the task. Decided to move forward with OSS and as part of P1 release evaluate Knowledge base. Documented this analysis and shared it with the engineering group and stakeholders so they are aware of our decisions. 
Result: successful launch of the chatbot on the agreed upon timeline. Timeline was imporant as Compensation has annual peaks in Q1. If we miss this peak then the next opportunity will be next year.
Do differently: While the initial implementation wasn't perfect it met the immediate needs of the business and allowed us to understand how to build a RAG service. The data processing components are still valuable and will be useful whenever we migrate to knowledge.


## Customer Obsession
### difficult interaction with a customer
### customer making unresonable request
A key customer, under pressure to deliver employee retention insights by Q2, requested enhancements to our feature engineering pipeline in February.  This was a late addition to their goals, and we hadn't planned for it in our Q1 roadmap.  Their goal was to identify high-performing employees at risk of attrition and provide managers with actionable recommendations.  They needed the new features to refine their predictive model and improve its accuracy. They explained that this validation was critical, as inaccurate predictions could lead to costly and ineffective retention interventions.

As the technical lead, I was responsible for assessing the impact of this request and determining a path forward.  My first step was to understand the why behind the customer's February timeline.  It turned out they needed time to validate the changes at their end and integrate the new features into their existing model.  I explained to the customer that while we were committed to supporting their goals, we had already prioritized other critical Q1 initiatives.  I emphasized the importance of evaluating trade-offs and ensuring that we could deliver on all our commitments.

After a thorough discussion, we negotiated a mid-March timeline.  This gave us time to properly plan and execute the work without jeopardizing our other Q1 goals.  I then deep-dived into the customer's request for 280 features.  I discovered that 75% of these features were essentially pivot values for different time periods – data we could generate dynamically.  I proposed a solution where we would initially create the granular base features, and then we would own the process of pivoting these on the fly, reducing the immediate workload and allowing for greater flexibility in the future.  The customer agreed to this approach.

I then took these refined requirements back to my team.  We collaboratively estimated the effort required, factoring in the complexity of the dynamic pivoting.  We finalized a six-week timeline, which we communicated transparently to the customer and all stakeholders.  I documented the details of our discussion, the agreed-upon timeline, and the technical approach in a one-pager, which I shared with everyone to ensure alignment and visibility.

The customer was able to validate their model using the new features before their Q2 deadline.  The dynamic pivoting solution we implemented allowed them to experiment with different time windows and ultimately improved the accuracy of their attrition predictions by X%. This led to a Y% improvement in employee retention within the pilot group.  We were able to deliver on our core commitments while still accommodating the customer's urgent need, albeit on a slightly adjusted timeline.

This experience reinforced the importance of proactive communication, collaborative problem-solving, and a deep understanding of the customer's business needs.  I learned that by focusing on the why behind the customer's request, I could better negotiate realistic timelines and find creative technical solutions that met both the customer's needs and our own team's priorities.  In the future, I will prioritize upfront discussions with customers to fully understand their business drivers and incorporate their needs into our planning process earlier.
### asked for customer feedback
Customer problem: confusion on ownership between us and data infrastructure team. when should customers reach out to which team.
Situation: We were the sole data team for science and analytics customers. after reorg we had another team that was branded as data infrastructure team. The teams didn't have clear lines of ownership of what use cases each team should be solving and customers were confused. We heard this feedback in our VoC meetings and from our leaders as well. 
Action: Created a 2 page proposal dilineating ownership between the 2 teams based on use cases. reviewed this proposal with both the teams and after agreement shared with with leaders and customers.
Result: no confusion with customers or leaders as we stopped hearing any anecdotes moving forward.
Do differently: could have been proactive. This confusion lasted for good 6 months before i wrote the proposal.
details of proposal
    * application sadie and analytics us. science had mixed use cases, so we divided them further.
    * batch and online models using feature store to us. graph based ML models sadie.
    * GenAI agents by sadie and RAG by us.

## Are Right a lot
### didn't have enough data to make the right decision
Customer Problem: Working on a goal to reduce the onboarding time of metrics to a reporting solution from 10 weeks to 3 weeks. In the current a different team owns vending our data in the reporting solution. The goal was to take ownership of this vending so we control the entire data stack to publish metrics to reporting solution.
Situation: To take ownership of the solution we started sessions with the team to understand it. They had built complex data pipelines comprising of 16 staging datasets, 1000s of line of SQL code. They did not have requirements captured for these as they has been created over time and under tight timelines from business. We had multiple back and forth with the teams to get the detailed requirements in a format that we preferred. But this was delaying the project as they had to dig through to extract the requirements and some of these requirements were not detailed/complete.
Action: After 3 iterations on the requirements and missing a requirement milestone by 3 weeks, dediced to start with what was provided, invest 1 week in analyzing their solution to identify any missing requirements. This still had the risk of missing requirements. So came up with a plan to pursue incremental testing of metrics completed (every 2 weeks) so we have a small feedback loop to identify gaps, aligned with the other team to continue to provide consulting support till the end of the project.
Result: Documented requirements for 330 metrics, able to complete design. Implementation will start in Q2.
Lesson: Since we were going to be the owners of this metrics it would have been good if we had a joint exercise of identifying requirements from the code instead of only pushing other team to provide it. Since they were extracting the requirements from the code it will have provided a good insights to our team in the process. Also we could have avoided the 3 week delay by jointly working.
### difficult decision with input from different stakeholders
Customer Problem: standardize the customer experience with access baseline across 8+ teams
Situation: We were execute an org level access baseline for the first time. As part of this we wanted to standardize the customer experience across 8+ teams and 1000s of customers. To achieve that we had to create a standard access request template and access review SOP to be used by all teams. The teams have used their own templates till now, which were effective to some extend but had gaps. The process to grant access was also independently defined by each team and had to be standardized.
Action: Reviewed existing templates and SOP to create a universal template that used existing best practices and created new standard ones to enable easier audit of baseline data, understanding of customer use cases. After reviewing with the teams, reviewed with teh principal engineers in the org and finally with a sample of customers.
Result: Overall reduced the number of followups. Were able to execute baseline across 8 teams in 10 weeks. Due to standardized request template was able to summarize the use cases of data access to leaders.
Do differently: use forms instead of ticket as customers often leave some fields blank. ALso summarizing 200+ use cases with LLMs was not effective and had to complement it with manual analysis.

## What i like about current role?
What I really appreciate about my current role is the balance of technical challenges, mentorship opportunities, strategic initiatives, and the impact I can have on the organization. I thrive on diving into complex technical problems, optimizing a key pipeline with process 3x data volumne without changing SLA.  Leading that effort, from design to implementation, was incredibly rewarding.  I also find a lot of satisfaction in mentoring junior engineers.  Guiding them and seeing their growth, like the junior engineer I helped with defining the data quality framework, is a highlight.  Finally, I value the ability to contribute to strategic decisions.  Being involved in technical roadmap discussions allows me to leverage my experience to influence the direction of our data platform.  These aspects of my current role align perfectly with what I'm looking for in a staff engineer position, and I'm eager to bring my skills and experience to [Company Name]
