+++
title = 'Behavorial'
date = 2025-02-02T17:42:35-08:00
draft = true
+++

## Introduction
Thank you for the opportunity to interview at XX. I have been working as a Senior Data Engineer in Amazon for the last 6 years and been with Amazon for last 10 years. I work in Talent Management Domain in HR space where I support analytsts and scienctists to help our leaders make data driven decisions for Amazon Employees. Some of my key achievements are creating a data vision for the org to improve data governance and speed of analytics for the org that impacted more than 8 teams. Led the development of the data governance strategy for the org and currently leading its implementation. Our team launched the first Retreival Augment Generation data service for an LLM that was used as a chatbot exposed to all Amazon employees. I am trusted by my managed to handle ambiguous or complex situations, ensure we solve the right customer problem and collaborate with other teams. I am interested in this role because XX. I believe my skills and experience make it a strong fit for this position.

## Why looking for a change?
I have learnt a lot at Amazon from data engineering, to data management at scale, to collaborating effectively across teams to solving the right problems. There are 2 reasons why i am looking for a chage. First my whole team is in Seattle and my management has shared that there is a good chance that I need to move to Seattle by next year. Since I want to stay in Bay Area i have a motivation to look for a new opportunity. Secondly I am seeking a role with bigger challenges and more responsibility. Reading the job description I felt this role provides that.

## Why are you interested in this role/company?
### Reddit
As a user of Reddit I have found it very helpful to deep dive on topics and get inputs from a community specific to my needs. I see it a platform build for authentic user communications. I will like to contribute to its mission to maintain the thriving community and be the source of valuable information for users. Also my past experience of supporting data driven decision making, working with product managers, scientists and analytics, build the data architecture for the org to improve the effectiveness of its decisions seems to be relevant to this role and I believe I can have good impact in this role. 




## Disagree and Commit
Customer Problem: Partnering with a principal to define a data strategy for the org to reduce risk of exposure to sensitive employee data and improve data consumer experience.
Situation: I was representing the data engineering and analytics community in the working group. The principal proposed a secure data vending strategy that i disagreed with. In my opinion it significantly impacted customer experience without providing then benefits. Also data governance is a cost to an organization and its a spectrum so there are alternate points for us to be at.
Action: Created a 1 pager to compare secure data vending with an alternate approach. Compared the 2 approaches on multiple dimensions like customer experience, cost, access management. They still did not agree. I requested them to review this with other DE teams and customers to get their inputs.
Results: When drafting the strategy they kept the proposed secure data vending. When it went for review with DE teams and customers there was more questions/pushback on the secure data vending and we revised this. For analytics we replaced secure data vending with some of the options I had proposed. For application data exchange we continued with secure data vending.
Do Differently: Involved the privacy specialist so they could vet my position on the spectrum of data governance and other points we could be on. Offline i could have connected with a few data engineers, collected feedback and shared with PE so we could incorporate this feedback early.

## Earn Trust
### Tough or critical feedback you received
Customer Problem: We had multiple instances of silent data quality issues in our datasets and customers had escalated. We wanted to improve our detection of such issues and prevent it from being published.
Situation: There were 2 sister teams and we had built a basic version of data quality monitoring using AWS deequ that helped us with data quality monitoring. The sister team was also trying to solve a similar problem. My manager asked me to see if they can leverage our solution. I took it as a challenge to influence them to use ours. I setup a meeting with them and focused on selling our solution to them. On all the approach they were exploring I will counter that will this can be done in our solution. After some time they went quiet. Later on my manager provided feedback that this was not the right way to work with other teams. we need to understand their situation, acknowledge their work and then try to see if we can identify value add for them to use our solution.
Action: Setup a meeting with them again and first acknowledged that my approach in last meeting to sell our soultion was not right. I want to first understand the current state and together we can come up with a solution that helps us address the problem. I listed to them, acknowledged their approach, explored possibilities and proposed value add on how they can save some of the time by using existing capabiltiy we had and proposed to improve our solution by them contributing to enhance it for their use cases.
Result: We jointly enhanced our data quality monitoring solution, used it for 30+ datasets that we own and 20+ datasets that they owned.

### Not able to meet a commitment
Customer Problem: We had a security incident where one of our customers had used sensitive employee data for a use case they were not approved for. It was escalated by a VP. When we did a deep dive we found they were granted access a few years back and never baselined. We never did baseline for access to our data and the same was true for multiple teams in our org.
Situation: Leaders in our org asked us to drive an initiative to baseline access across all data products in the org. There were about 340 datasets and 160 dashboards. We provided an estimate of 3 weeks, 1 week to prep and 2 week to execute. When we created a detailed plan we realized its going to take us more time to prepare and we proposed a timeline of 5 weeks. When we actually went about executing the baseline it took us about 12 weeks to complete. Some of the challenges were delays from customer end to recertify their access, conflicting priorities on teams in our org, lack of preparedness by the teams in our org to execute it.
Actions: We kept sending out weekly flash to leaders so they were aware of the process, challenges and what we were doing to address them. Among other teams in the org we were the biggest vendors of data. So we shared best practices we followed and learnings with other teams.
Result: Successful completion of the access baseline. We documented practices, learnings to stremline this process moving forward. This was also an input to the org wide data strategy on what platforms should be leveraged so we can automate the baseline.

### Uncovered a significant problem in the team/ improved team's productivity
Customer problem: We were puting out 20% to 45% of hte tasks in our sprints and didn't have a good understanding why. There were multiple occurence of missing timelines committed to customers. We always had some specific reasons but i believed we were not planning/executing the tasks appropriatly. 
Situation:
Actions: I analyzed past 8 sprints and realized we were punting 20% to 45% of the tasks. Our sprint retro was too generic with team discussing unrelated challenges. Even our standup process was not helping us understand if we are on track on the sprint tasks or delayed. I collected all this data and created a 2 pager proposal. It analytized our current state and identified the root cause as team working on stories before they are groomed and also team not doing an effective sprint retro to learn from past experience and prevent it. I proposed a new agenda for sprint retro that included specific section where each person analyzes what they are punting, why and what they can do to prevent it in future. I also proposed a definition of done for grooming so team can focus on execution of the task when its included in sprint instead of clarifying requirements.
Result: After following this process for a quarter, we observed number of punted tasks falling below 10%, we had specific learnings and understanding of why we are punting tasks. In the subsequent voice of customer meeting we received anecdotal evidence from customers on an improvement on our team meeting timelines.

### Helped a team member who was struggling or impacted my work
Customer Problem: our team was working on a proposal to improve the data quality of our datasets, which we can later use to improve data quality for other teams in the org.
Situation: An engineer was assigned this project and they were working with their manager on this. When we reviewed the initial proposal it focussed too much of the tooling but not enough on the root cause for the current state of data quality in the org. I asked them to meet with all teams in our org and revise the proposal. In the next revision of the proposal it still focused on tooling instead of the root cause.
Action: As a senior member of the team i recognized their struggles and offered to mentor them. My goal was not just to solve the immediate problem but also to equip them with the skills and knowledge needed to handle similar challenge in future. Instead of giving them a solution i guided them through the problem solving  process. I explained to them before we propose any tooling we should be able to describe in 1 page what is the root cause of lack of data quality monitoring in the org and if tooling is the gap we should provide evidence for that. I reviewed the interview notes from other teams and they were focused on discussion about what capabilities teams want for data quality monitoring. I setup meeting with 2 of the teams again and led the discussion so they could understand what kind of information we collect to diagnose the current situation.
Result: They were able to collect meaningful insights from the rest of the teams. Together we identified lack of data quality monitoring in the org was due to lack of understanding of what should one monitor as part of data quality. We created the proposal that defines what kind of data quality issues are, what are the different sources to understand what is critical to monitor, what are the different approaches to monitor for such issues. In the end we leveraged existing tooling instead of building our own. We completed monitoring on 35 datasets owned by us and reviewed this proposal with other teams in the org showing evidence of this strategy being successful. Currently have helped 1 more team in the org to improve their data quality  monitoring. We are still working with other teams to improve their data quality monitoring.
Do differently: Customer interviews I should have been part of hte first round of intervies so we did not have to go through them again. At the beginning of the initiative set the expectations and been more explicit on how they should draft the proposal, template they should use. 

## Bias for Action
### Calculated risk where speed was critical
Customer Problem: One of our datasets which was considered the oxygen for talent management analytics had not had a change in 4 years. When we did introduce a change we released a bug that introduced a data quality issue. The customer's were unable to run their analytics, sone of them relied on this data to implement Fine Grain Access Control on their data.
Situation: We identified the root cause as the bug and decided to rollback to the previous state. Then we realized that we didn't have backup of the previous data. The engineer had executed a backfill in production that had overridden all checkpoint files, staging table and final output. So we did not have any data to restore it from. So we have a situation to restore data quickly but don't have a clear SOP to do it.
Action: I setup a small working group to help restore this. I evaluated potential risks of proceeding without an SOP, considered cost of introducing a new bug, potential impact on customers and likelihood of it happenning. I decided to not proceed without an SOP. In the short turn around time I decided to create a simple version of an SOP to restore pipeline and data to previous state. This still had the risk of not being tested and peer reviewed by the team. To mitigate the risk   I peer reviewed with 1 senior engineer and 1 other engineer. We created small data testing script for each stage of the pipeline to ensure we catch any issues with restore early.
Result: Restored data in 2 days and customers were unblocked. The validating scripts we created helped us catch 1 issue during the restore. The calculated risk of using a non tested SOP allowed us to restore the pipeline and data on a short turnaround time, thus unblocking customers. I authored a CoE after this incident.
Do differently: we could have assigned an individual to work with the platform team to restore the last version of the final output. that would have mitigated the customer impact while we fixed the issue and make the pipeline ready for next run.

### Decision without consulting your manager
Customer Problem: Create child goals for leadership to review.
Situation: We support science and analytics customers and some of our goals work back from their goals. One of the goals was experimentation. We had clarity on some of their work like streamling experimentation analysis that we created goals on while for other goals like creating north star metrics for measuresment there was ambiguity on ownership, scope of work.
Action: i pushed back not scoping out work for the north star metrics till we have more clarity. they said that last year we had a similar goal that was ambiguous but we created a goal. I did acknowledge that but mentioned it is hard to create a goal unless we have a clear understanding of impact, why its important and how it will be measured.
Result: didn't create goal and later on discussed with manager and they aligned. had more followups with the science team and helped them define ownership for those metrics with the data engineering team that support that product.
Do differently: nothing different but there were good lessons here, once you have a clear understanding of what is required to set goals it is easier to have tough discussions on why we are not doing it

### Tight deadline and didn't have time to consider all options
Customer problem: launching a chatbot for Amazon employees to ask compensation related questions. we owned creating the Retreival Augmented Generation Service to provide the LLM amazon specific compensation data.
Situation: We did a PoC on 3 Vector databases Opensearch serverless, Amazon Kendra and Amazon RDS with vector plugin. We finalized OSS due to its low cost of ownership  for our use case. During implementation Amazon Bedrock accounced an integrated RAG service Knowledge base. As a technical lead i was responsible for the data architecture. I know we don't have time to make a detailed assessment of Knowledge base before launching. I had to make a decision on continuing our path or moving to knowledge base.
Action: did a quick 1 day document read of Knowledge base. Realized there were still open questions related to support for text + vector searches, latency and TPS limits. it was hard to finalize this without doing an additional PoC. I recommended to use what we were already familiar with and which was well suited for the task. Decided to move forward with OSS and as part of P1 release evaluate Knowledge base. Documented this analysis and shared it with the engineering group and stakeholders so they are aware of our decisions. 
Result: successful launch of the chatbot on the agreed upon timeline. Timeline was imporant as Compensation has annual peaks in Q1. If we miss this peak then the next opportunity will be next year.
Do differently: While the initial implementation wasn't perfect it met the immediate needs of the business and allowed us to understand how to build a RAG service. The data processing components are still valuable and will be useful whenever we migrate to knowledge.


## Customer Obsession
### difficult interaction with a customer
### customer making unresonable request
Customer problem: Customer had a tight deadline and were under presure to delivery a goal by Q2. They were generating new insights on identifying high performing employees who were at risk of attrition to be able to recommend appropriate action to their mangers.
Situation: For the goal customer required support to enhance the feature engineering pipeline in Feb. This was a late addition to their goal so we did not plan out any work on this in Q1. This was about  weeks of effort.
Action: As at technical lead I was responsible for assessing the impact and identifying a path forward. First tried to understand the rationale for Feb timeline by customers. it turns out this was to provide them buffer to validate changes at their end. First I explained to customer that we had already planned for Q1 and these were important goals for us. thus we need to evalute tradeoffs when prioritizing this work. I negotiated and agreed upon mid March timeline. Then deep dived into the request for 280 features and found 75% of them were pivot values for different time periods. Negotiated in the short term for us to created granular features and then to own pivoting these on the fly. I took these requirements back to the team to come up with an accurate estimate. We  finalized 6 weeks and communicated the same to the customers. Documented this discussion in a 1 pager that I shared with all stakeholders to ensure people have visibility into the decisions we took.
Result: Customers were unblocked and were able to validate the model using new features before launch. We were able to make a comprimise, focus on delivering the core functionality on time. 
Lesson: This was a good lesson in active listening, clear communication and collaborative problem solving when dealing with tough customer requests.

### asked for customer feedback
Customer problem: confusion on ownership between us and data infrastructure team. when should customers reach out to which team.
Situation: We were the sole data team for science and analytics customers. after reorg we had another team that was branded as data infrastructure team. The teams didn't have clear lines of ownership of what use cases each team should be solving and customers were confused. We heard this feedback in our VoC meetings and from our leaders as well. 
Action: Created a 2 page proposal dilineating ownership between the 2 teams based on use cases. reviewed this proposal with both the teams and after agreement shared with with leaders and customers.
Result: no confusion with customers or leaders as we stopped hearing any anecdotes moving forward.
Do differently: could have been proactive. This confusion lasted for good 6 months before i wrote the proposal.
details of proposal
    * application sadie and analytics us. science had mixed use cases, so we divided them further.
    * batch and online models using feature store to us. graph based ML models sadie.
    * GenAI agents by sadie and RAG by us.

## Are Right a lot
### didn't have enough data to make the right decision


### difficult decision with input from different stakeholders
Customer Problem: standardize the customer experience with access baseline across 8+ teams
Situation: We were execute an org level access baseline for the first time. As part of this we wanted to standardize the customer experience across 8+ teams and 1000s of customers. To achieve that we had to create a standard access request template and access review SOP to be used by all teams. The teams have used their own templates till now, which were effective to some extend but had gaps. The process to grant access was also independently defined by each team and had to be standardized.
Action: Reviewed existing templates and SOP to create a universal template that used existing best practices and created new standard ones to enable easier audit of baseline data, understanding of customer use cases. After reviewing with the teams, reviewed with teh principal engineers in the org and finally with a sample of customers.
Result: Overall reduced the number of followups. Were able to execute baseline across 8 teams in 10 weeks. Due to standardized request template was able to summarize the use cases of data access to leaders.
Do differently: use forms instead of ticket as customers often leave some fields blank. ALso summarizing 200+ use cases with LLMs was not effective and had to complement it with manual analysis.

